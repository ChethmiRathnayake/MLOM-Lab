{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f40424",
   "metadata": {},
   "source": [
    "Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f3252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33288f5",
   "metadata": {},
   "source": [
    "Load ascii text and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e038643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:\\\\Users\\\\rmct2\\\\OneDrive - Sri Lanka Institute of Information Technology\\\\Desktop\\\\SLIIT\\\\MLOM lab\\\\data.txt\"\n",
    "raw_text = open(filename,'r',encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290c7d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"project gutenberg's alice's adventures in wonderland, by lewis carroll\\n\\nthis ebook is for the use of\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55397a6",
   "metadata": {},
   "source": [
    "We must prepare the data for modeling by the neural network. We cannot model the charachters directly. Instead we must convert the character value(integer)\n",
    "\n",
    "We can do this easily by first creating a set of all the distinct characters in the book, then creating a map of each character to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "406c9d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#create mapping of unique char to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)\n",
    "char_to_int = dict((c,i) for i,c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64a3883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e010919",
   "metadata": {},
   "source": [
    "Geting the details of the dataset\n",
    "\n",
    "There are just 150,000 characters and that when converted to lowercase that there are only 58 distinct characters in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31b2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163780\n",
      "Total Vocab(Unique characters):  58\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \",n_chars)\n",
    "print(\"Total Vocab(Unique characters): \",n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c6d69",
   "metadata": {},
   "source": [
    "Prepare the dataset of input to output pairs encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d157911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163765\n"
     ]
    }
   ],
   "source": [
    "#Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 15 #can be changed\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length,1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataY)\n",
    "print(\"Total Patterns: \",n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c974b",
   "metadata": {},
   "source": [
    "Transform the list of input sequences into the form [samples, time steps, features] that is expected by an LSTM\n",
    "network and rescale the integers to the range [0,1] to make the patterns easier to learn by the LSTM network\n",
    "that uses the sigmoid activation function by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312131e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.81034483]\n",
      "  [0.84482759]\n",
      "  [0.79310345]\n",
      "  ...\n",
      "  [0.77586207]\n",
      "  [0.56896552]\n",
      "  [0.62068966]]\n",
      "\n",
      " [[0.84482759]\n",
      "  [0.79310345]\n",
      "  [0.70689655]\n",
      "  ...\n",
      "  [0.56896552]\n",
      "  [0.62068966]\n",
      "  [0.84482759]]\n",
      "\n",
      " [[0.79310345]\n",
      "  [0.70689655]\n",
      "  [0.62068966]\n",
      "  ...\n",
      "  [0.62068966]\n",
      "  [0.84482759]\n",
      "  [0.65517241]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.55172414]\n",
      "  [0.56896552]\n",
      "  [0.79310345]\n",
      "  ...\n",
      "  [0.79310345]\n",
      "  [0.79310345]\n",
      "  [0.72413793]]\n",
      "\n",
      " [[0.56896552]\n",
      "  [0.79310345]\n",
      "  [0.89655172]\n",
      "  ...\n",
      "  [0.79310345]\n",
      "  [0.72413793]\n",
      "  [0.86206897]]\n",
      "\n",
      " [[0.79310345]\n",
      "  [0.89655172]\n",
      "  [0.87931034]\n",
      "  ...\n",
      "  [0.72413793]\n",
      "  [0.86206897]\n",
      "  [0.22413793]]]\n"
     ]
    }
   ],
   "source": [
    "#reshape X to be[samples,time steps,features]\n",
    "X = numpy.reshape(dataX,(n_patterns, seq_length, 1))\n",
    "#normalize - rescaling the integer values\n",
    "X = X / float(n_vocab)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df2942",
   "metadata": {},
   "source": [
    "**Note** - We use the dropout to obtain generalization of the dataset instead of overfitting the training dataset perferctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed038f52",
   "metadata": {},
   "source": [
    "Convert the output values (single characters converted to integers) into a one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76f20cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243e3ee",
   "metadata": {},
   "source": [
    "Define the LSTM mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ff5b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmct2\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))#It can have 1 or more training samples\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam')\n",
    "#define the checkpoints\n",
    "filepath = \"weights-improvement-{epoch:02d} - {loss:.2f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c027341",
   "metadata": {},
   "source": [
    "Fitting model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27455d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chang the hyperparameter values and train model\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0072011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.0795\n",
      "Epoch 1: loss improved from inf to 2.98125, saving model to weights-improvement-01 - 2.98.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 3.0793\n",
      "Epoch 2/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.8229\n",
      "Epoch 2: loss improved from 2.98125 to 2.79872, saving model to weights-improvement-02 - 2.80.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.8228\n",
      "Epoch 3/10\n",
      "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.7325\n",
      "Epoch 3: loss improved from 2.79872 to 2.71531, saving model to weights-improvement-03 - 2.72.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.7324\n",
      "Epoch 4/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.6681\n",
      "Epoch 4: loss improved from 2.71531 to 2.66177, saving model to weights-improvement-04 - 2.66.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - loss: 2.6681\n",
      "Epoch 5/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.6249\n",
      "Epoch 5: loss improved from 2.66177 to 2.61667, saving model to weights-improvement-05 - 2.62.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.6249\n",
      "Epoch 6/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.5711\n",
      "Epoch 6: loss improved from 2.61667 to 2.57067, saving model to weights-improvement-06 - 2.57.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - loss: 2.5711\n",
      "Epoch 7/10\n",
      "\u001b[1m1278/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.5378\n",
      "Epoch 7: loss improved from 2.57067 to 2.52574, saving model to weights-improvement-07 - 2.53.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.5378\n",
      "Epoch 8/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.4813\n",
      "Epoch 8: loss improved from 2.52574 to 2.48003, saving model to weights-improvement-08 - 2.48.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.4813\n",
      "Epoch 9/10\n",
      "\u001b[1m1279/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.4481\n",
      "Epoch 9: loss improved from 2.48003 to 2.43787, saving model to weights-improvement-09 - 2.44.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.4481\n",
      "Epoch 10/10\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.4003\n",
      "Epoch 10: loss improved from 2.43787 to 2.40019, saving model to weights-improvement-10 - 2.40.keras\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - loss: 2.4003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x270186e3580>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs=epochs, batch_size = batch_size, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d065c",
   "metadata": {},
   "source": [
    "Generate Text with the trained LSTM mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d73695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163765\n",
      "144909\n",
      "Seed: \n",
      "\" a pleasure in a \"\n"
     ]
    }
   ],
   "source": [
    "#load the network weights\n",
    "filename = \"weights-improvement-10 - 2.40.keras\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "#generate a random seed\n",
    "print(len(dataX))\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "print(start)\n",
    "pattern = dataX[start] #dataX contains list of patterns\n",
    "print(\"Seed: \")\n",
    "print(\"\\\"\",''.join([int_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac6495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
